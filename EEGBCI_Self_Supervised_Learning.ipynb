{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEGBCI Self-Supervised Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxDpf+NuGcZoqSZtUpjvsO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshualin24/EEG_demo/blob/main/EEGBCI_Self_Supervised_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Supervised Learning on EEGBCI Motor Movement/Imagery Dataset"
      ],
      "metadata": {
        "id": "Vy0Muyj3er_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook trains self-supervised learning models on the [EEGBCI dataset](http://www.ncbi.nlm.nih.gov/pubmed/15188875) on [PhysioNet](https://www.ahajournals.org/doi/10.1161/01.CIR.101.23.e215), including using the [SeqCLR](https://proceedings.mlr.press/v136/mohsenvand20a.html) and the [DINO](https://arxiv.org/abs/2104.14294) frameworks. Detailed data description and experimental protocol can be found [here](https://physionet.org/content/eegmmidb/1.0.0/#files-panel)."
      ],
      "metadata": {
        "id": "QQBmlvmbex_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisite"
      ],
      "metadata": {
        "id": "9t1YJt9zf1Ni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5hkstMfegHn"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mne\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import pytorch_lightning as pl\n",
        "import torchaudio\n",
        "import random\n",
        "from typing import Optional, Callable\n",
        "from collections.abc import Sequence"
      ],
      "metadata": {
        "id": "LCKMNkKMf8DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some global variables and hyperparameters:"
      ],
      "metadata": {
        "id": "lhLu9aOigBAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVENT_ID = {  # integer identifier of experimental tasks/events\n",
        "    'rest/eye-open': 0,\n",
        "    'rest/eye-closed': 1,\n",
        "    'movement/left/fist': 2,\n",
        "    'movement/right/fist': 3,\n",
        "    'imagery/left/fist': 4,\n",
        "    'imagery/right/fist': 5,\n",
        "    'movement/both/fist': 6,\n",
        "    'movement/both/foot': 7,\n",
        "    'imagery/both/fist': 8,\n",
        "    'imagery/both/foot': 9\n",
        "}"
      ],
      "metadata": {
        "id": "ET3jPOV8f_wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation"
      ],
      "metadata": {
        "id": "E6wJarqJgPOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_windows(\n",
        "    raw: mne.io.Raw,\n",
        "    subject: int,\n",
        "    run: int,\n",
        "    event_id: dict,\n",
        "    duration: float = 4.0,\n",
        "    baseline_duration: float = 0.2\n",
        "):\n",
        "    \"\"\"\n",
        "    Return windows/epochs extracted from raw data.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    raw: Raw data\n",
        "    subject: Subject of recording\n",
        "    run: Experimental run\n",
        "    event_id: Mapping from tasks/events to integer identifiers\n",
        "    duration: duration of windows that capture responses to tasks/events\n",
        "    baseline_duration: duration of baseline for drift correction\n",
        "    \n",
        "    Note\n",
        "    ----\n",
        "    Baseline segments are not discarded from the returned windows.\n",
        "    \"\"\"\n",
        "    events, focal_event_id = get_events(raw, run, event_id)\n",
        "    metadata = pd.DataFrame({\n",
        "        'start': events[:, 0] / raw.info['sfreq'],  # start time of tasks\n",
        "        'task': events[:, -1],  # identifier of experimental tasks\n",
        "        'subject': subject,\n",
        "        'run': run\n",
        "    })\n",
        "    event_mapping = {ind: key for key, ind in event_id.items()}\n",
        "    return mne.Epochs(\n",
        "        raw,\n",
        "        events,\n",
        "        event_id={event_mapping[ind]: ind for ind in focal_event_id.values()},\n",
        "        tmin=-baseline_duration,\n",
        "        tmax=duration,\n",
        "        metadata=metadata\n",
        "    )\n",
        "\n",
        "\n",
        "def get_events(\n",
        "    raw: mne.io.Raw,\n",
        "    run: int,\n",
        "    event_id: dict,\n",
        "    fixed_length_duration: float = 4.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Return tuple of events and their integer-identifier-mapping according to\n",
        "    the experimental run.\n",
        "\n",
        "    Argmuments\n",
        "    ----------\n",
        "    raw: Raw data\n",
        "    run: Experimental run\n",
        "    event_id: Mapping from tasks/events to integer identifiers\n",
        "    fixed_length_duration: Duration to create fixed-length events\n",
        "\n",
        "    Note\n",
        "    ----\n",
        "    For runs with alternating experimental tasks, the rest-state periods in\n",
        "    between will not be considered.\n",
        "    \"\"\"\n",
        "    if run in (1, 2):\n",
        "        events = mne.make_fixed_length_events(\n",
        "            raw,\n",
        "            id=run,\n",
        "            duration=fixed_length_duration\n",
        "        )\n",
        "        return events, {'T0': run}\n",
        "    elif run in (3, 7, 11):\n",
        "        event_id = {\n",
        "            'T1': event_id['movement/left/fist'],\n",
        "            'T2': event_id['movement/right/fist']\n",
        "        }\n",
        "        return mne.events_from_annotations(raw, event_id=event_id)\n",
        "    elif run in (4, 8, 12):\n",
        "        event_id = {\n",
        "            'T1': event_id['imagery/left/fist'],\n",
        "            'T2': event_id['imagery/right/fist']\n",
        "        }\n",
        "        return mne.events_from_annotations(raw, event_id=event_id)\n",
        "    elif run in (5, 9, 13):\n",
        "        event_id = {\n",
        "            'T1': event_id['movement/both/fist'],\n",
        "            'T2': event_id['movement/both/foot']\n",
        "        }\n",
        "        return mne.events_from_annotations(raw, event_id=event_id)\n",
        "    elif run in (6, 10, 14):\n",
        "        event_id = {\n",
        "            'T1': event_id['imagery/both/fist'],\n",
        "            'T2': event_id['imagery/both/foot']\n",
        "        }\n",
        "        return mne.events_from_annotations(raw, event_id=event_id)\n",
        "    else:\n",
        "        raise ValueError('invalid experimental run.')\n",
        "\n",
        "\n",
        "def eegbci_epochs(\n",
        "    subjects: list,\n",
        "    runs: list,\n",
        "    dir: str = './',\n",
        "    select: Optional[str] = None,\n",
        "    **kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "    Return extracted windows/epochs from the EEGBCI motor movement/imagery data.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    subjects: List of subjects of interest\n",
        "    runs: List of experimental runs for each subject\n",
        "    dir: Root directory path to the EEGBCI data\n",
        "    select: Key to select a part of extracted windows/epochs\n",
        "    kwargs: Keyword arguments passed to extract windows\n",
        "    \n",
        "    Note\n",
        "    ----\n",
        "    Baseline segments are not discarded from the returned windows.\n",
        "    \"\"\"\n",
        "    epochs_list = [\n",
        "        extract_windows(\n",
        "            mne.io.read_raw_edf(mne.datasets.eegbci.load_data(subject, run, path=dir)[0]),\n",
        "            subject,\n",
        "            run,\n",
        "            EVENT_ID,\n",
        "            **kwargs\n",
        "        ) for subject in subjects for run in runs\n",
        "    ]\n",
        "    epochs = mne.concatenate_epochs(epochs_list)\n",
        "    return epochs if select is None else epochs[select]"
      ],
      "metadata": {
        "id": "Exbp8pOygf8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGBCIDataset(Dataset):\n",
        "    \"\"\"Dataset of the EEGBCI motor movement/imagery data.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        subjects: list,\n",
        "        runs: list,\n",
        "        get_label: Callable,\n",
        "        transform: Callable = None,\n",
        "        target_transform: Callable = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        subjects: List of subjects of interest\n",
        "        runs: List of experimental runs for each subject\n",
        "        get_label: Function to extract label from metadata\n",
        "        transform: Transformation upon data\n",
        "        target_transform: Transformation upon labels\n",
        "        kwargs: Keyword arguments passed to extract windows\n",
        "        \n",
        "        Note\n",
        "        ----\n",
        "        Baseline segments are discarded from the returned windows.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.windows = eegbci_epochs(subjects, runs, **kwargs)\n",
        "        self.get_label = get_label\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.windows)\n",
        "    \n",
        "    def __getitem__(self, ind: int):\n",
        "        window = self.windows[ind]\n",
        "        data = window.get_data(tmin=0.0)[0]  # baseline segment discarded\n",
        "        if self.transform: data = self.transform(data)\n",
        "        label = self.get_label(window.metadata)\n",
        "        if self.target_transform: label = self.target_transform(label)\n",
        "        return data, label"
      ],
      "metadata": {
        "id": "4bqyK9rEggsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentations & Transformations"
      ],
      "metadata": {
        "id": "9Q3esvNZhBXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DCShift:\n",
        "    \"\"\"Add a random constant shift to data.\"\"\"\n",
        "\n",
        "    def __init__(self, min_shift: float, max_shift: float):\n",
        "        self.base = min_shift\n",
        "        self.scale = max_shift - min_shift\n",
        "\n",
        "    def __call__(self, data: torch.Tensor):\n",
        "        \"\"\"Data must have dimensions `(..., time steps)`.\"\"\"\n",
        "        shifts = self.base + self.scale * torch.rand_like(data[..., :1])\n",
        "        return data + shifts\n",
        "\n",
        "\n",
        "class AmplitudeScale:\n",
        "    \"\"\"Scale data with a randomly selected constant.\"\"\"\n",
        "\n",
        "    def __init__(self, min_scale: float, max_scale: float):\n",
        "        self.base = min_scale\n",
        "        self.scale = max_scale - min_scale\n",
        "\n",
        "    def __call__(self, data: torch.Tensor):\n",
        "        \"\"\"Data must have dimensions `(..., time steps)`.\"\"\"\n",
        "        scales = self.base + self.scale * torch.rand_like(data[..., :1])\n",
        "        return data * scales\n",
        "\n",
        "\n",
        "class AdditiveGaussianNoise:\n",
        "    \"\"\"Add Gaussian noise with a random standard deviation.\"\"\"\n",
        "\n",
        "    def __init__(self, min_sigma: float, max_sigma: float):\n",
        "        self.base = min_sigma\n",
        "        self.scale = max_sigma - min_sigma\n",
        "\n",
        "    def __call__(self, data: torch.Tensor):\n",
        "        \"\"\"Data must have dimensions `(..., time steps)`.\"\"\"\n",
        "        sigmas = self.base + self.scale * torch.rand_like(data[..., :1])\n",
        "        return data + torch.randn_like(data) * sigmas\n",
        "\n",
        "\n",
        "class TimeMasking:\n",
        "    \"\"\"Mask data for a randomly selected time segment.\"\"\"\n",
        "\n",
        "    def __init__(self, min_len: int, max_len: int, mask_val: float = 0.0):\n",
        "        self.base = min_len\n",
        "        self.scale = max_len - min_len\n",
        "        self.val = mask_val\n",
        "    \n",
        "    def __call__(self, data: torch.Tensor):\n",
        "        \"\"\"Data must have dimensions `(..., time steps)`.\"\"\"\n",
        "        lens = self.base + self.scale * torch.rand_like(data[..., :1])\n",
        "        starts = (data.size(-1) - lens) * torch.rand_like(data[..., :1])\n",
        "        ends = starts + lens\n",
        "        inds = torch.arange(data.size(-1), dtype=data.dtype, device=data.device)\n",
        "        return data.masked_fill((inds >= starts) & (inds < ends), self.val)\n",
        "\n",
        "\n",
        "class TimeShift:\n",
        "    \"\"\"Shift data along the time axis with a periodic boundary condition.\"\"\"\n",
        "\n",
        "    def __init__(self, min_shift: int, max_shift: int):\n",
        "        self.base = min_shift\n",
        "        self.scale = max_shift - min_shift\n",
        "    \n",
        "    def __call__(self, data: torch.Tensor):\n",
        "        \"\"\"Data must have dimensions `(..., time steps)`.\"\"\"\n",
        "        shifts = self.base + self.scale * torch.rand_like(data[..., :1])\n",
        "        shifts = torch.remainder(shifts, data.size(-1))\n",
        "        # Prepare indices for masking\n",
        "        inds = torch.arange(data.size(-1), dtype=data.dtype, device=data.device)\n",
        "        inds_rev = data.size(-1) - 1 - inds\n",
        "        # Allocate and fill in shifted data\n",
        "        shifted = torch.empty_like(data)\n",
        "        shifted[inds < shifts] = data[inds_rev < shifts]\n",
        "        shifted[inds >= shifts] = data[inds_rev >= shifts]\n",
        "        return shifted\n",
        "\n",
        "\n",
        "class BandStopFilter:\n",
        "    \"\"\"Filter out data for a frequency band.\"\"\"\n",
        "\n",
        "    def __init__(self, min_freq, max_freq, bandwidth, sample_rate):\n",
        "        self.base = min_freq\n",
        "        self.scale = max_freq - min_freq\n",
        "        self.bandwidth = bandwidth\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def __call_(self, data: torch.Tensor):\n",
        "        \"\"\"Data must have dimensions `(..., time steps)`.\"\"\"\n",
        "        central_freqs = self.base + self.scale * torch.rand_like(data[..., :1])\n",
        "        return torchaudio.functional.bandreject_biquad(\n",
        "            data,\n",
        "            self.sample_rate,\n",
        "            central_freqs,\n",
        "            central_freqs / self.bandwidth\n",
        "        )"
      ],
      "metadata": {
        "id": "QDKoEkvghFAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomAugmentationPair:\n",
        "    \"\"\"Randomly selected pair of augmentations from a given collection.\"\"\"\n",
        "\n",
        "    def __init__(self, augmentations: Sequence):\n",
        "        self.augs = augmentations\n",
        "    \n",
        "    def __call__(self, data: torch.Tensor):\n",
        "        \"\"\"Return tuple of augmented data.\"\"\"\n",
        "        aug_1, aug_2 = random.sample(self.augs, 2)\n",
        "        return aug_1(data), aug_2(data)"
      ],
      "metadata": {
        "id": "9ZnemjkmhhgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Supervised Learning Frameworks & Models"
      ],
      "metadata": {
        "id": "MyIkfPe0hmbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SeqCLR (adopted from SimCLR)\n",
        "\n",
        "See reference [here](https://proceedings.mlr.press/v136/mohsenvand20a.html):\n",
        "\n",
        "Mohsenvand, M.N., Izadi, M.R. &amp; Maes, P.. (2020). Contrastive Representation Learning for Electroencephalogram Classification. <i>Proceedings of the Machine Learning for Health NeurIPS Workshop</i>, in <i>Proceedings of Machine Learning Research</i> 136:238-253"
      ],
      "metadata": {
        "id": "seq0e5m0htrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Constrative loss function in the SimCLR framework.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    1. Full credit to https://zablo.net/blog/post/understanding-implementing-simclr-guide-eli5-pytorch/.\n",
        "    2. See also Chen, Ting, et al. \"A simple framework for contrastive learning\n",
        "       of visual representations.\" International conference on machine learning.\n",
        "       PMLR, 2020.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size: int, temperature: float = 0.5):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        batch_size: Batch size (as well as the size of candidate pool)\n",
        "        temperature: Parameter of sharpness\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.register_buffer(\"temperature\", torch.tensor(temperature))\n",
        "        self.register_buffer(\n",
        "            \"negatives_mask\",\n",
        "            (~torch.eye(batch_size*2, batch_size*2, dtype=bool)).float()\n",
        "        )\n",
        "            \n",
        "    def forward(self, emb_i: torch.Tensor, emb_j: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Return the Boltzmann distribution of positive pairs, where the energy\n",
        "        function is the cosine similarity between embeddings.\n",
        "        \"\"\"\n",
        "        z_i = torch.nn.functinal.normalize(emb_i, dim=1)\n",
        "        z_j = torch.nn.functinal.normalize(emb_j, dim=1)\n",
        "        representations = torch.cat([z_i, z_j], dim=0)\n",
        "        # Compute cosine similarity between positive pairs\n",
        "        similarity_matrix = torch.nn.functinal.cosine_similarity(\n",
        "            representations.unsqueeze(1),\n",
        "            representations.unsqueeze(0),\n",
        "            dim=2\n",
        "        )\n",
        "        sim_ij = torch.diag(similarity_matrix, self.batch_size)\n",
        "        sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n",
        "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
        "        # Compute loss value\n",
        "        nominator = torch.exp(positives / self.temperature)\n",
        "        denominator = self.negatives_mask * torch.exp(similarity_matrix / self.temperature)\n",
        "        loss_partial = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
        "        loss = torch.sum(loss_partial) / (2 * self.batch_size)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "9x_Uz2pVh0RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqCLR(pl.LightningModule):\n",
        "    \"\"\"Contrastive learning on individual channels of EEG data.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder: nn.Module,\n",
        "        projector: nn.Module,\n",
        "        augmentations: Sequence,\n",
        "        batch_size: int,\n",
        "        temperature: float = 5e-2,\n",
        "        learning_rate: float = 1e-4,\n",
        "        weight_decay: float = 1e-4\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        encoder: Encoder module that extract representation from data\n",
        "        projector: Projection head module\n",
        "        augmentations: Collection of augmentations of interests\n",
        "        batch_size: Batch size of training/validation set\n",
        "        temperature: Temperature of contrastive loss\n",
        "        learning_rate: Learning rate of optimizer\n",
        "        weight_decay: Weight decay of optimizer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.projector = projector\n",
        "        self.augmentation_pair = RandomAugmentationPair(augmentations)\n",
        "        self.criterion = ContrastiveLoss(batch_size, temperature)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "    \n",
        "    def forward(self, data: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Return the learned representation of data.\n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Data must have dimensions `(batch, channels, ..., time steps)`.\n",
        "        \"\"\"\n",
        "        repr = self.encoder(data.flatten(end_dim=1))\n",
        "        return repr.unflatten(0, data.size()[:2])\n",
        "    \n",
        "    def training_step(self, batch, batch_ind):\n",
        "        loss = self._shared_step(batch, batch_ind)\n",
        "        self.log('Train/Loss', loss)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_ind):\n",
        "        loss = self._shared_step(batch, batch_ind)\n",
        "        self.log('Validation/Loss', loss)\n",
        "    \n",
        "    def _shared_step(self, batch, batch_ind):\n",
        "        \"\"\"Return loss of the current batch.\"\"\"\n",
        "        data, = batch\n",
        "        data = data.flatten(end_dim=1)  # flatten channels into the batch\n",
        "        augmented_1, augmented_2 = self.augmentation_pair(data)\n",
        "        loss = self.criterion(\n",
        "            self.projector(self.encoder(augmented_1)),\n",
        "            self.projector(self.encoder(augmented_2))\n",
        "        )\n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.learning_rate,\n",
        "            weight_decay=self.weight_decay\n",
        "        )\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "LKSPPZ7RiACE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DINO\n",
        "\n",
        "See reference [here](https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html):\n",
        "\n",
        "Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin; Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 9650-9660"
      ],
      "metadata": {
        "id": "I1Gu-LnyiFtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StudentTeacherLoss(nn.Module):\n",
        "    \"\"\"Cross-entropy loss between student and teacher outputs after softmax.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_std_augs: int,\n",
        "        num_tch_augs: int,\n",
        "        student_temperature: float,\n",
        "        teacher_temperature: float,\n",
        "        momentum: float = 0.9\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        dim: Output dimension of student/teacher module\n",
        "        num_std_augs: Number of augmentations passed to student module\n",
        "        num_tch_augs: Number of augmentations passed to teacher module\n",
        "        student_temperature: Sharpness parameter for student output\n",
        "        teacher_temperature: Sharpness parameter for teacher output\n",
        "        momentum: Update rate parameter of center buffer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.register_buffer('center', torch.zeros(dim))\n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "            (~torch.eye(num_tch_augs, num_std_augs, dtype=bool)).float()\n",
        "        )\n",
        "        self.std_tpr = student_temperature\n",
        "        self.tch_tpr = teacher_temperature\n",
        "        self.momentum = momentum\n",
        "\n",
        "    def forward(self, std_out: torch.Tensor, tch_out: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        std_out: Output of student module\n",
        "        tch_out: Output of teacher module\n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Output of student/teacher module must have dimension\n",
        "        `(batch, augmentations, projection/embedding)`.\n",
        "        \"\"\"\n",
        "        mask = self.mask  # to exclude same augmentation in student/teacher\n",
        "        for _ in range(len(std_out.size())-2): mask = mask.unsqueeze(-1)\n",
        "        tch_prob = torch.nn.functional.softmax(\n",
        "            (tch_out - self.center) / self.tch_tpr,\n",
        "            dim=-1\n",
        "        ).unsqueeze(2)\n",
        "        std_log_prob = torch.nn.functional.log_softmax(\n",
        "            std_out / self.std_tpr,\n",
        "            dim=-1\n",
        "        ).unsqueeze(1)\n",
        "        loss = (-mask * tch_prob * std_log_prob).flatten(start_dim=1).sum(1)\n",
        "        self.update_center(tch_out)\n",
        "        return loss\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def update_center(self, tch_out: torch.Tensor):\n",
        "        \"\"\"Update center buffer used for teacher output.\"\"\"\n",
        "        self.center *= self.momentum\n",
        "        self.center += (1 - self.momentum) * tch_out.flatten(end_dim=-2).mean(0)"
      ],
      "metadata": {
        "id": "_QGCFVdHiJbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DINO(pl.LightningModule):\n",
        "    \"\"\"Self-supervised learning motivated by knowledge distillation.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        student: nn.Module,\n",
        "        teacher: nn.Module,\n",
        "        student_head: nn.Module,\n",
        "        teacher_head: nn.Module,\n",
        "        common_augs: tuple,\n",
        "        std_exclusive_augs: tuple,\n",
        "        projection_dim: int,\n",
        "        student_temperature: float,\n",
        "        teacher_temperature: float,\n",
        "        teacher_momentum: float,\n",
        "        center_momentum: float,\n",
        "        learning_rate: float,\n",
        "        weight_decay: float\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        student: Encoder of student module\n",
        "        teacher: Encoder of teacher module\n",
        "        student_head: Projection head of student module\n",
        "        teacher_head: Projection head of teacher module\n",
        "        common_augs: Augmentations passed to both student and teacher modules\n",
        "        std_exclusive_augs: Augmentations passed to student module only\n",
        "        projection_dim: Output dimension of projection head\n",
        "        student_temperature: Sharpness parameter for student output\n",
        "        teacher_temperature: Sharpness parameter for teacher output\n",
        "        teacher_momentum: Update rate parameter of teacher module\n",
        "        center_momentum: Update rate parameter of dummy centers\n",
        "        learning_rate: Learning rate of student module\n",
        "        weight_decay: Weight decay when training student module\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        1. `teacher`/`teacher_head` must have the same architecture as\n",
        "           `student`/`student_head`.\n",
        "        2. `common_augs` and `std_exclusive_augs` should be mutually exclusive.\n",
        "        3. `projection_dim` must match the dimension of output of\n",
        "           `student_head`/`teacher_head`.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.student = nn.Sequential(student, student_head)\n",
        "        self.teacher = nn.Sequential(teacher, teacher_head)\n",
        "        self.encoder = student\n",
        "        self.std_augs = common_augs + std_exclusive_augs\n",
        "        self.tch_augs = common_augs\n",
        "        self.momentum = teacher_momentum\n",
        "        self.criterion = StudentTeacherLoss(\n",
        "            projection_dim,\n",
        "            len(self.std_augs),\n",
        "            len(self.tch_augs),\n",
        "            student_temperature,\n",
        "            teacher_temperature,\n",
        "            center_momentum\n",
        "        )\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        # Force teacher and student module to start with the same weights\n",
        "        self.teacher.load_state_dict(self.student.state_dict())\n",
        "        # Disable gradient tracking for teacher module\n",
        "        for p in self.teacher.parameters(): p.requires_grad_(False)\n",
        "\n",
        "    def forward(self, data: torch.Tensor):\n",
        "        \"\"\"Return the learned representation of data.\"\"\"\n",
        "        return self.encoder(data)\n",
        "    \n",
        "    def training_step(self, batch, batch_ind):\n",
        "        # Manually update student module\n",
        "        opt = self.optimizers()\n",
        "        opt.zero_grad()\n",
        "        loss = self._shared_step(batch, batch_ind)\n",
        "        loss.manual_backward()\n",
        "        opt.step()\n",
        "        # Update teacher module with exponential moving average\n",
        "        for p_tch, p_std in zip(\n",
        "            self.teacher.parameters(),\n",
        "            self.student.parameters()\n",
        "        ):\n",
        "            p_tch.mul_(self.momentum).add_(p_std.detach(), alpha=1.0-self.momentum)\n",
        "        # Log metric\n",
        "        self.log('Train/Loss', loss)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_ind):\n",
        "        loss = self._shared_step(batch, batch_ind)\n",
        "        self.log('Validation/Loss', loss)\n",
        "    \n",
        "    def _shared_step(self, batch, batch_ind):\n",
        "        \"\"\"Return loss of the current batch.\"\"\"\n",
        "        data, = batch\n",
        "        augmented_std = torch.cat(  # with dimension (batch, augmentation, ...)\n",
        "            [aug(data) for aug in self.std_augs],\n",
        "            dim=1\n",
        "        ).unflatten(1, (len(self.std_augs), -1))\n",
        "        augmented_tch = torch.cat(  # with dimension (batch, augmentation, ...)\n",
        "            [aug(data) for aug in self.tch_augs],\n",
        "            dim=1\n",
        "        ).unflattn(1, (len(self.tch_augs), -1))\n",
        "        loss = self.criterion(\n",
        "            self.student(augmented_std),\n",
        "            self.teacher(augmented_tch).detach()\n",
        "        )\n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.AdamW(  # optimizer to train student module\n",
        "            self.student.parameters(),\n",
        "            lr=self.learning_rate,\n",
        "            weight_decay=self.weight_decay\n",
        "        )\n",
        "        return opt"
      ],
      "metadata": {
        "id": "bAgFIA_ZiRc3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}